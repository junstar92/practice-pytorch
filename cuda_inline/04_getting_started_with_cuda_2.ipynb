{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "797b2bd4-ac6d-4da4-b169-9f77793a056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.cpp_extension import load_inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc0188d-4428-40b0-a4ff-84f11fa31300",
   "metadata": {},
   "source": [
    "# Approximate gelu as a fusion example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe42ad2d-1514-4068-b331-74e93e7cb6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    return 0.5 * x * (1 + torch.tanh((2 / torch.pi)**0.5 * (x + 0.044715 * x**3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1e3ec89-06bf-4779-ba69-42f8e4e916f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1024, 1024, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37d53e6f-7b3b-4411-884e-67ba3b0e55fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gelu(x) - torch.nn.functional.gelu(x, approximate='tanh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f9f6ee5-ea08-477a-aa4e-0a499afe98d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118 µs ± 206 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "20.3 µs ± 323 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit gelu(x); torch.cuda.synchronize()\n",
    "%timeit torch.nn.functional.gelu(x, approximate='tanh'); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8712109-4f42-454c-97a4-77719f75052e",
   "metadata": {},
   "source": [
    "## Kind of slow. Why ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "300b9d3a-14e4-4a4f-8252-80a9eb0a98c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_begin = r'''\n",
    "#include <torch/extension.h>\n",
    "#include <stdio.h>\n",
    "#include <c10/cuda/CUDAException.h>\n",
    "\n",
    "#define CHECK_CUDA(x) TORCH_CHECK(x.device().is_cuda(), #x \" must be a CUDA tensor\")\n",
    "#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n",
    "#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n",
    "\n",
    "inline unsigned int cdiv(unsigned int a, unsigned int b) { return (a + b - 1) / b;}\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2522287a-4efa-4be9-b282-0a544935001f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_src = cuda_begin + r\"\"\"\n",
    "__global__ void my_gelu_kernel(float* __restrict__ out, float* __restrict__ in, int n) {\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    if (i >= n) return;\n",
    "    float x = in[i];\n",
    "    out[i] = 0.5f * x * (1.f + tanhf(sqrtf(2.f / 3.141592653589793f) * (x + 0.044715f * (x * x * x))));\n",
    "}\n",
    "\n",
    "torch::Tensor my_gelu_out(torch::Tensor output, torch::Tensor const& in) {\n",
    "    CHECK_INPUT(in);\n",
    "    int n = in.numel();\n",
    "    TORCH_CHECK((output.sizes() == in.sizes()) || (output.device() == in.device()) || (output.scalar_type() == in.scalar_type()));\n",
    "\n",
    "    int threads = 256;\n",
    "    my_gelu_kernel<<<cdiv(n, threads), threads>>>(\n",
    "        output.data_ptr<float>(), in.data_ptr<float>(), n);\n",
    "    \n",
    "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "    return output;\n",
    "}\n",
    "\n",
    "torch::Tensor my_gelu(torch::Tensor const& in) {\n",
    "    CHECK_INPUT(in);\n",
    "    auto output = torch::empty_like(in);\n",
    "    my_gelu_out(output, in);\n",
    "    return output;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "cpp_src = \"\"\"\n",
    "torch::Tensor my_gelu(torch::Tensor const& in);\n",
    "torch::Tensor my_gelu_out(torch::Tensor output, torch::Tensor const& in);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b28358b7-c940-41ff-9432-e5c5a61a68e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "gelu_module = load_inline(\n",
    "    \"test_ext_gelu\", cpp_src, cuda_src,\n",
    "    functions=['my_gelu', 'my_gelu_out'],\n",
    "    extra_cuda_cflags=['--ptxas-options=-v'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f15824b-9860-4e51-b5be-15d6ce20485e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3842e-07, device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(gelu_module.my_gelu(x) - gelu(x)).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9d37eb2-6522-43c1-8da9-c3fa78a13dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.5 µs ± 146 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit gelu_module.my_gelu(x); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb72f5b-c944-4a07-8760-2ca7ba827d99",
   "metadata": {},
   "source": [
    "# Measure launch latency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d2778e4-85e2-4f5d-8a2f-445bec4a6d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_src = cuda_begin + r'''\n",
    "__global__ void my_empty_kernel(float* __restrict__ out, float* __restrict__ in, int n) {\n",
    "}\n",
    "\n",
    "torch::Tensor my_empty_out(torch::Tensor output, torch::Tensor const& in) {\n",
    "    CHECK_INPUT(in);\n",
    "    int n = in.numel();\n",
    "    TORCH_CHECK((output.sizes() == in.sizes())  || (output.device() == in.device()) || (output.scalar_type() == in.scalar_type()));\n",
    "    \n",
    "    int threads = 256;\n",
    "    my_empty_kernel<<<cdiv(n, threads), threads>>>(\n",
    "        output.data_ptr<float>(), in.data_ptr<float>(), n);\n",
    "        \n",
    "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "    return output;\n",
    "}\n",
    "\n",
    "torch::Tensor my_empty(torch::Tensor const& in) {\n",
    "    CHECK_INPUT(in);\n",
    "    auto output = torch::empty_like(in);\n",
    "    my_empty_out(output, in);\n",
    "    return output;\n",
    "}\n",
    "'''\n",
    "\n",
    "cpp_src = \"\"\"\n",
    "torch::Tensor my_empty(torch::Tensor const& in);\n",
    "torch::Tensor my_empty_out(torch::Tensor output, const torch::Tensor& in);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "63693cb8-4c05-43c0-b0aa-95e94a2ac6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_module = load_inline(\n",
    "    \"test_ext_empty\", cpp_src, cuda_src,\n",
    "    functions=['my_empty', 'my_empty_out'],\n",
    "    extra_cuda_cflags=['--ptxas-options=-v'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e1424b50-045d-4d13-856e-fb1aa1ac55a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 µs ± 13 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "STAGE:2024-03-13 18:40:24 678474:678474 ActivityProfilerController.cpp:311] Completed Stage: Warm Up\n",
      "STAGE:2024-03-13 18:40:24 678474:678474 ActivityProfilerController.cpp:317] Completed Stage: Collection\n",
      "STAGE:2024-03-13 18:40:24 678474:678474 ActivityProfilerController.cpp:321] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                                    Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg    # of Calls  \n",
      "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                        cudaLaunchKernel        74.39%      33.327ms        74.39%      33.327ms       3.333us       0.000us         0.00%       0.000us       0.000us         10000  \n",
      "    my_empty_kernel(float*, float*, int)         0.00%       0.000us         0.00%       0.000us       0.000us      40.244ms       100.00%      40.244ms       4.024us         10000  \n",
      "                   cudaDeviceSynchronize        25.61%      11.471ms        25.61%      11.471ms       1.147us       0.000us         0.00%       0.000us       0.000us         10001  \n",
      "----------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 44.798ms\n",
      "Self CUDA time total: 40.244ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%timeit empty_module.my_empty_out(x, x); torch.cuda.synchronize()\n",
    "\n",
    "with torch.profiler.profile() as prof:\n",
    "    for i in range(10_000):\n",
    "        empty_module.my_empty_out(x, x)\n",
    "        torch.cuda.synchronize()\n",
    "print(prof.key_averages().table())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560111c4-84ab-4d2d-9122-57c1a03fb641",
   "metadata": {},
   "source": [
    "# Tiled Matmul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b86262b-e249-4557-8e63-f2ff64c62de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_src = cuda_begin + r\"\"\"\n",
    "__global__ void simple_matmul_kernel(float* __restrict__ a, float* __restrict__ b, float* __restrict__ out, int m, int n, int k) {\n",
    "    int r = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int c = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "\n",
    "    if (r >= m || c >= n) return;\n",
    "    float tmp = 0.f;\n",
    "    for (int i = 0; i < k; ++i) {\n",
    "        tmp += a[r * k + i] * b[i * n + c];\n",
    "    }\n",
    "    out[r * n + c] = tmp;\n",
    "}\n",
    "\n",
    "torch::Tensor simple_matmul(torch::Tensor a, torch::Tensor b) {\n",
    "    CHECK_INPUT(a);\n",
    "    CHECK_INPUT(b);\n",
    "    int m = a.size(0);\n",
    "    int n = b.size(1);\n",
    "    int k = a.size(1);\n",
    "    TORCH_CHECK(k == b.size(0), \"Size mismatch!\");\n",
    "\n",
    "    auto output = torch::zeros({m, n}, a.options());\n",
    "\n",
    "    dim3 threads_per_block{16, 16};\n",
    "    dim3 blocks{cdiv(n, threads_per_block.x), cdiv(m, threads_per_block.y)};\n",
    "\n",
    "    simple_matmul_kernel<<<blocks, threads_per_block>>>(\n",
    "        a.data_ptr<float>(), b.data_ptr<float>(), output.data_ptr<float>(), m, n, k);\n",
    "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "\n",
    "    return output;\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "cpp_src = \"torch::Tensor simple_matmul(torch::Tensor a, torch::Tensor b);\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6fadeafb-7c7a-4c59-94fc-486b29794319",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_matmul_module = load_inline(\n",
    "    \"test_ext_simple_matmul\", cpp_src, cuda_src, \n",
    "    functions=['simple_matmul'], extra_cuda_cflags=['--ptxas-options=-v'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "65130b68-f55b-4d9f-9e86-c87ad545f337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11 ms ± 1.58 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.0002, device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(1024, 1024, device=\"cuda\")\n",
    "b = torch.randn(1024, 1024, device=\"cuda\")\n",
    "%timeit simple_matmul_module.simple_matmul(a, b)\n",
    "\n",
    "(simple_matmul_module.simple_matmul(a, b) - a@b).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5321c845-e89a-4c73-8253-c571739b586d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_src = cuda_begin + r\"\"\"\n",
    "constexpr int TILE_SIZE = 16;\n",
    "\n",
    "__global__ void tiled_matmul_kernel(float* out, float* M, float* N, int m, int n, int k) {\n",
    "    __shared__ float M_tile[TILE_SIZE][TILE_SIZE];\n",
    "    __shared__ float N_tile[TILE_SIZE][TILE_SIZE];\n",
    "    \n",
    "    // idxes into tile\n",
    "    int ir = threadIdx.y;\n",
    "    int ic = threadIdx.x;\n",
    "    \n",
    "    int r = blockIdx.y * blockDim.y + threadIdx.y;\n",
    "    int c = blockIdx.x * blockDim.x + threadIdx.x;\n",
    "    \n",
    "    // note: cannot just exit if we want to do padding!\n",
    "    \n",
    "    float res = 0.0f;\n",
    "    for (int K_tileidx = 0; K_tileidx < (k + TILE_SIZE -1) / TILE_SIZE; K_tileidx++) {\n",
    "        // note how threadIdx.x is the fastes moving bit --> coalesced memory access\n",
    "        M_tile[ir][ic] = (((r < m) && (K_tileidx * TILE_SIZE + ic < k)) ? M[r * k + K_tileidx * TILE_SIZE + ic] : 0.f);\n",
    "        N_tile[ir][ic] = ((((K_tileidx * TILE_SIZE + ir) < k) && (c < n)) ? N[(K_tileidx * TILE_SIZE + ir) * n + c] : 0.f);\n",
    "        __syncthreads();\n",
    "        for (int idx = 0; idx < TILE_SIZE; idx++) {\n",
    "            res += M_tile[ir][idx] * N_tile[idx][ic];\n",
    "        }\n",
    "        __syncthreads(); // important! (why?)\n",
    "    }\n",
    "    if ((r < m) && (c < n)) {\n",
    "        out[r * n + c] = res;\n",
    "    }\n",
    "}\n",
    "\n",
    "torch::Tensor tiled_matmul(torch::Tensor const& a, torch::Tensor const& b) {\n",
    "    CHECK_INPUT(a); CHECK_INPUT(b);\n",
    "    int m = a.size(0);\n",
    "    int n = b.size(1);\n",
    "    int k = a.size(1);\n",
    "    TORCH_CHECK(k==b.size(0), \"Size mismatch\");\n",
    "    \n",
    "    auto output = torch::empty({m, n}, a.options());\n",
    "\n",
    "    dim3 tpb{TILE_SIZE, TILE_SIZE};\n",
    "    dim3 blocks{cdiv(n, tpb.x), cdiv(m, tpb.y)};\n",
    "    tiled_matmul_kernel<<<blocks, tpb>>>(\n",
    "        output.data_ptr<float>(), a.data_ptr<float>(), b.data_ptr<float>(), m, n, k);\n",
    "\n",
    "    C10_CUDA_KERNEL_LAUNCH_CHECK();\n",
    "    return output;\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "cpp_src = \"\"\"\n",
    "torch::Tensor tiled_matmul(torch::Tensor const& m, torch::Tensor const& n);\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d02d2eed-fa6e-45bc-8912-f0b39c456eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiled_matmul_module = torch.utils.cpp_extension.load_inline(\n",
    "    \"test_ext_tiled_matmul\", cpp_src, cuda_src, \n",
    "    functions=['tiled_matmul'], extra_cuda_cflags=['--ptxas-options=-v'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f923138b-895c-496a-b3ea-c96bb7aa3fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit tiled_matmul_module.tiled_matmul(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21f4ffe-0580-4c7b-9951-fae1a072a7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = torch.randn(500, 200, device=\"cuda\")\n",
    "bb = torch.randn(200, 1000, device=\"cuda\")\n",
    "\n",
    "(tiled_matmul_module.tiled_matmul(aa, bb) - aa@bb).abs().max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5dd92f-4b9b-4fcb-8f26-d7cff132c492",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
